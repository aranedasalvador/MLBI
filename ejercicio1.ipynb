#%% [markdown]
## importando los modulos requeridos
#%%
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import logging
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
logging.getLogger("tensorflow").setLevel(logging.WARNING)


import tensorflow as tf
from tensorflow import keras
print(tf.__version__)

import numpy as np
import pandas as pd
#%% [markdown]
## Se lee el data set
#%%
data_1=pd.read_excel(r'C:\Users\Joel\Downloads\Machine learning for bussines intelligence\Tarea_1\dataset1.xlsx')
data_1.head(10)



from sklearn.preprocessing import LabelEncoder , StandardScaler , OneHotEncoder
#Label enconder para agregar etiquetas numericas a la columna categorica crr_Nom
le = LabelEncoder()
le.fit(data_1['crr_Nom'])
data_1['crr_Nom']=le.transform(data_1['crr_Nom'])

data_1=data_1.sample(frac=1) #random shuffle data

data_x=data_1.loc[:,:'MUNICIPAL']
y=data_1['cluster']

#Se suavizan (normalizan) los valores de x con StandarScaler
scaler = StandardScaler()
x=scaler.fit_transform(data_x)

#Onehotencoder para y
y=np.array(y)
y=y.reshape(-1, 1)
oneHot = OneHotEncoder() 
oneHot.fit(y) 
y = oneHot.transform(y).toarray() 

#TamaÃ±o de train y test
data_size = data_1.shape[0]
train_pct = 0.8
train_size = int(data_size * train_pct)

x_train, y_train = x[:train_size], y[:train_size] #x_train e y_train son  numpy.array
x_test, y_test = x[train_size:], y[train_size:]    #x_test and y_test are  numpy.arrays  

#%% [markdown]
##logistic regression model as class:
#%%

class Model(object):
    def __init__(self, dim):
    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)
        samples, features = dim
        hidden1_nodes = 4
        hidden2_nodes = 1
        self.theta1 = tf.Variable(tf.random.normal([features+1,hidden1_nodes], dtype= tf.float64) ,name = "Theta1")
        self.theta2 = tf.Variable(tf.random.normal([hidden1_nodes+1,hidden2_nodes], dtype= tf.float64), name = "Theta2")
        #bias1 = tf.constant(tf.ones(x.shape[0],1), dtype=tf.float64, name='bias1')
        #bias2 = tf.constant(tf.ones(x.shape[0],1), dtype=tf.float64, name='bias2')
        
        self.trainable_variables = [self.theta1, self.theta2]
        
        
    def __call__(self, x):
        samples = x.shape[0]
        self.bias1 = tf.constant(1, dtype=tf.float64, shape=(samples,1), name='bias1')
        self.bias2 = tf.constant(1, dtype=tf.float64, shape=(samples,1), name='bias2')
        a0 = tf.concat([self.bias1,x],1, name='a0')
        z1 = tf.matmul(a0,self.theta1, name='z1')
        a1 = tf.concat([self.bias2,tf.sigmoid(z1)],1,name='a1')
        z2 = tf.matmul(a1,self.theta2, name='z2')
        a2 = tf.nn.softmax(z2, name='a2')
        return a2

def loss(target_y, predicted_y):
    return  tf.reduce_mean(-tf.reduce_sum(target_y * np.log(predicted_y)))
    #-tf.reduce_sum(target_y*tf.math.log(predicted_y)+(1-target_y)*tf.math.log(1-predicted_y), axis = 0, name='Cost_function')

def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        current_loss = loss(outputs, model(inputs))
    dThe1, dThe2 = t.gradient(current_loss, [model.theta1, model.theta2])
    model.theta1.assign_sub(learning_rate * dThe1)
    model.theta2.assign_sub(learning_rate * dThe2)

def lr_schedule(epoch):
    """
    Returns a custom learning rate that decreases as epochs progress.
    """
    learning_rate = 0.2
    if epoch > 3000:
        learning_rate = 0.15
    if epoch > 4000:
        learning_rate = 0.1
    if epoch > 4500:
        learning_rate = 0.05
    return learning_rate


#%% [markdown]
### Optimize
    
#%%
# Declare the inputs
x = tf.constant(x_train, dtype=tf.float64, name='x')
y = tf.constant(y_train, dtype=tf.float64, name='y')

model = Model(dim=x.shape)
theta1_hist, theta2_hist = [], []
for epoch in range(5000):
    current_loss = loss(y, model(x))
    learning_rate = lr_schedule(epoch)
    train(model, x, y, learning_rate=learning_rate)
    if epoch % 100 == 0:
        theta1_hist.append(model.theta1.numpy())
        theta2_hist.append(model.theta2.numpy())
        print('Epoch %2d: learning_rate=%2.5f, loss=%2.5f' % (epoch, learning_rate, current_loss))

#%% [markdown]
#Confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)
#%%
#%% [markdown]
#f1score
from sklearn.metrics import f1_score
f1_score =f1_score(y_test, predictions, average='macro')
print(f1_score)
#%%

