#%% [markdown]
## importando los modulos requeridos
#%%
import tensorflow as tf
from tensorflow import keras
print(tf.__version__)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler 
#%% [markdown]
## Se lee el data set

#%%
data_2 = pd.read_csv("Tarea_1\dataset2.csv")
data_2.head(10)
#%% [markdown]
## Se establecens las matrices x e y 

#%%
y=data_2['condition']
x=data_2.drop('condition',axis=1)
#%% [markdown]
## Se normalizan la matriz x

#%%
scaler = StandardScaler()
x=scaler.fit_transform(x)
#%% [markdown]
## Se determinan los conjuntos de entrenamiento y testeo, donde el tama√±o de testes es el 20%

#%%
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)

class Model(object):
    def __init__(self, x):
        # Initialize variable to (5.0, 0.0)
        # In practice, these should be initialized to random values.
        self.Weight = tf.Variable((tf.cast(tf.ones(shape=[3,1]), dtype=tf.float64)))
        self.Bias = tf.constant(tf.zeros([1]),dtype=tf.float64) 
        self.trainable_variables = [self.Weight]

    def __call__(self, x):
        ypred = tf.nn.sigmoid(tf.add(tf.matmul(x, self.Weight),self.Bias))
        y_hat=ypred
        return y_hat


alpha, epochs = 0.0035, 500 
print('Learning Rate =', alpha) 
print('Number of Epochs =', epochs)

     
# Sigmoid Cross Entropy Cost Function 
cost = tf.nn.sigmoid_cross_entropy_with_logits( 
                    logits = Y_hat, labels = Y) 
  
# Gradient Descent Optimizer 
optimizer = tf.train.GradientDescentOptimizer( 
         learning_rate = alpha).minimize(cost) 




